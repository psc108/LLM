# LLM Assistant Configuration

# Flask settings
HOST=0.0.0.0
PORT=5000
FLASK_DEBUG=false
SECRET_KEY=change-this-in-production

# Ollama settings
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
MODEL_NAME=codellama:13b-instruct
